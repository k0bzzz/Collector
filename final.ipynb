{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb08bc7-1ea9-4860-8679-ce522ada7b5a",
   "metadata": {},
   "source": [
    "## Библиотеки и глобальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "ac88efaa-47a2-4c3b-8823-4b77f3569dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pymorphy2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import Counter\n",
    "import lightgbm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from umap.umap_ import UMAP\n",
    "import torch\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('russian') + stopwords.words('english')\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e04220ca-c32a-41a7-b1f0-7f6a22c80f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "BERT = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "MORPH = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e1c47-949e-43ac-919c-bc71eadf62c5",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "03378fb0-ae0e-4270-8b6d-1fbbd3e6ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues_train = pd.read_csv(\"data/train_issues.csv\")\n",
    "df_comment_train = pd.read_csv(\"data/train_comments.csv\")\n",
    "\n",
    "df_issues_test = pd.read_csv(\"data/test_issues.csv\")\n",
    "df_comment_test = pd.read_csv(\"data/test_comments.csv\")\n",
    "\n",
    "df_emp = pd.read_csv(\"data/employees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "34cffd58-5a23-4471-92d2-9f86a62394dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues_test['overall_worklogs'] = -1\n",
    "df_all = pd.concat([df_issues_train, df_issues_test]).reset_index(drop=True)\n",
    "df_all_com = pd.concat([df_comment_train, df_comment_test]).reset_index(drop=True)\n",
    "df_all_com.text = df_all_com.text.apply(lambda x: x.split(':')[-1] if x.__contains__('mentioned this issue in') else x) # Чистим комментарии от автотекста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10a2d2-d11a-47a3-9534-757fd96850af",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "cb4a26f3-a983-4c0d-804d-67eb7033e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОБЪЕДИНЕНИЕ ВСЕХ ТЕКСТОВЫХ ДАННЫХ\n",
    "def concat_all_text(X):    \n",
    "    com_df = pd.merge(X, df_all_com, left_on=\"id\", right_on=\"issue_id\", how='left')\n",
    "    com_df.text = com_df.text.fillna('')\n",
    "    all_text = {}\n",
    "    for val in X.id:\n",
    "        all_text[val] = com_df[com_df.id == val].summary.iloc[0]\n",
    "        all_text[val] += ' '.join([text for text in com_df[com_df.id == val].text.values])\n",
    "    X['all_text'] = all_text.values()\n",
    "    \n",
    "# ДАТА И ВРЕМЯ\n",
    "def encode_date(X):\n",
    "    print('processisng date...')\n",
    "    X.created = pd.to_datetime(X.created)\n",
    "    X['year'] = X.created.dt.year\n",
    "    X['day'] = X.created.dt.day\n",
    "    X['month'] = X.created.dt.month\n",
    "    X['hour'] = X.created.dt.hour\n",
    "    X['day_of_week'] = X.created.dt.strftime(\"%w\").astype(int)\n",
    "    X.drop('created', axis=1, inplace=True)\n",
    "\n",
    "# КОЛИЧЕСТВО И ДЛИНА КОММЕНТАРИЕВ\n",
    "def process_coments(X):\n",
    "    print('processisng comments...')\n",
    "    com_df = pd.DataFrame(pd.merge(X, df_all_com, left_on=\"id\", right_on=\"issue_id\", how='left'))\n",
    "    com_df.text = com_df.text.astype('str')\n",
    "    com_df.text = com_df.text.apply(len).astype('int')\n",
    "    counts = com_df.groupby('id').comment_id.count()\n",
    "    lens = com_df.groupby('id').text.sum()\n",
    "    X['comments_count'] = pd.merge(X, counts, left_on='id', right_index=True).comment_id\n",
    "    X['comments_len'] = pd.merge(X, lens, left_on='id', right_index=True).text\n",
    "    \n",
    "# ИНФОРМАЦИЯ ОБ ИСПОЛНИТЕЛЕ\n",
    "def get_assignee_info(X):\n",
    "    print('processisng assignee info...')\n",
    "    df = pd.merge(X, df_emp, left_on='assignee_id', right_on='id', how='left')\n",
    "    df.fillna('unknown', inplace=True)\n",
    "    col_list = ['position', 'hiring_type', 'payment_type', 'passport', 'is_labor_contract_signed', 'is_added_one_to_one']\n",
    "    X[col_list] = df[col_list]\n",
    "\n",
    "# ПОЛУЧЕНИЕ СТАТИСТИК\n",
    "def get_stats(X, field):\n",
    "    print(f'getting {field.split(\"_\")[0]} statistics...')\n",
    "    train_df = X[X.overall_worklogs != -1]\n",
    "    stats_df = train_df.groupby(field).agg({'overall_worklogs': [min, max, np.mean, np.median]})\n",
    "    stats_df.columns = [f'{field.split(\"_\")[0]}_{i}_time' for i in stats_df.columns.droplevel(0)]\n",
    "    X[stats_df.columns] = pd.merge(X, stats_df, left_on=field, right_index=True, how='left')[stats_df.columns]\n",
    "    \n",
    "# TF-IDF UMAP\n",
    "def tf_idf(freqs):\n",
    "    return (freqs / (freqs.sum(axis=1) + 0.0001)[:, np.newaxis] ) * np.log(freqs.shape[0] / ((freqs >= 1).sum(axis=0) + 1))\n",
    "\n",
    "def preprocess(text: str) -> list:\n",
    "    tokenized = nltk.word_tokenize(text.lower())\n",
    "    normalized = [MORPH.parse(word)[0].normal_form for word in tokenized\n",
    "                  if len(word) > 1 and word.isalpha()]\n",
    "    return normalized\n",
    "\n",
    "def to_vec(vocab: dict, text):\n",
    "    vec = np.zeros(len(vocab) + 1)\n",
    "    for token in text:\n",
    "        if token in vocab:\n",
    "            vec[vocab[token]] += 1\n",
    "        else:\n",
    "            vec[0] += 1\n",
    "    return vec\n",
    "\n",
    "def get_vocab(vals):\n",
    "    vocab = {}\n",
    "    pos = 1\n",
    "    cc = Counter()\n",
    "    for text in vals:\n",
    "        n_cc = Counter(text)\n",
    "        for key in n_cc:\n",
    "            n_cc[key] = 1\n",
    "        cc += n_cc\n",
    "    for key, val in cc.items():\n",
    "        if val > 1:\n",
    "            vocab[key] = pos\n",
    "            pos += 1\n",
    "    return vocab\n",
    "\n",
    "def get_tf_idf_enc(vals, X, col):\n",
    "    print('TF-IDF encoding...')\n",
    "    vocab = get_vocab(vals)\n",
    "    all_freq = []\n",
    "    for val in vals:\n",
    "        all_freq.append(to_vec(vocab, val))\n",
    "    all_freq = np.array(all_freq)\n",
    "    \n",
    "    all_freq = tf_idf(all_freq)\n",
    "    reduct = UMAP(densmap=True,\n",
    "                  dens_lambda=1.,\n",
    "                  n_neighbors=10,\n",
    "                  min_dist=0.0,\n",
    "                  n_components=5,\n",
    "                  random_state=42,\n",
    "                  low_memory=False,\n",
    "                  metric='euclidean',\n",
    "                  output_metric='euclidean'\n",
    "                  )\n",
    "\n",
    "    umap_res = reduct.fit_transform(all_freq).T\n",
    "    for i in range(len(umap_res)):\n",
    "        X[col + f'_umap_{i}'] = umap_res[i]\n",
    "\n",
    "# BERT UMAP\n",
    "def bert(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "def get_bert_enc(X):   \n",
    "    embeddings = []\n",
    "    print('making BERT embeddings...')\n",
    "    for sentence in tqdm(X['all_text']):\n",
    "        embeddings.append(bert(sentence, BERT, TOKENIZER))\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    reduct = UMAP(densmap=True,\n",
    "                  dens_lambda=1.,\n",
    "                  n_neighbors=10,\n",
    "                  min_dist=0.0,\n",
    "                  n_components=5,\n",
    "                  random_state=42,\n",
    "                  low_memory=False,\n",
    "                  metric='euclidean',\n",
    "                  output_metric='euclidean'\n",
    "                  )\n",
    "\n",
    "    umap_res = reduct.fit_transform(embeddings).T\n",
    "    for i in range(len(umap_res)):\n",
    "        X[f'bert_umap_{i}'] = umap_res[i]\n",
    "\n",
    "# КАТЕГОРИАЛЬНЫЕ ПРИЗНАКИ\n",
    "def make_categorical(df, col):\n",
    "    df[col] = pd.Categorical(df[col])\n",
    "    df[col] = df[col].astype('category').cat.codes\n",
    "\n",
    "# УНИКАЛЬНЫЕ СЛОВА В ЗАДАНИЯХ С ВЫСОКИМ ТАРГЕТОМ  \n",
    "def unique_words(lower=0, upper=df_issues_train.overall_worklogs.max()):\n",
    "    X = df_issues_train[df_issues_train.overall_worklogs.between(lower, upper)].copy()\n",
    "    X['clean_text'] = X.summary.apply(preprocess)\n",
    "    vocab = get_vocab(X.clean_text)\n",
    "    vocab = pd.DataFrame({'word': vocab.keys(), 'freq': vocab.values()})\n",
    "    \n",
    "    all_txt = []\n",
    "    texts = [all_txt + i for i in X.clean_text]\n",
    "    freqs=[]\n",
    "    for lst in texts:\n",
    "        freqs +=lst\n",
    "    freqs = Counter(freqs)\n",
    "    freqs = pd.DataFrame({'word': freqs.keys(), 'freq': freqs.values()})\n",
    "    freqs = freqs[~freqs.word.isin(stop_words)]\n",
    "    return freqs\n",
    "\n",
    "def check_unique_words(lst):\n",
    "    for word in lst:\n",
    "        if word in merged.word.values:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# РАЗНИЦА ВО ВРЕМЕНИ МЕЖДУ ДВУМЯ БЛИЖАЙШИМИ ЗАДАНИЯМИ У КАЖДОГО ИСПОЛНИТЕЛЯ\n",
    "def get_time_difference(X):\n",
    "    print('getting time differences since last task...')\n",
    "    res = pd.DataFrame(columns=['id', 'time_diff'])\n",
    "    X.created = pd.to_datetime(X.created)\n",
    "    for df in X.sort_values('created', ascending=False).expanding():\n",
    "        curr = df.iloc[-1]\n",
    "        try: \n",
    "            prev = df[df.assignee_id == curr.assignee_id].iloc[-2]\n",
    "            time_diff = pd.Timedelta(curr.created-prev.created).seconds\n",
    "        except IndexError:\n",
    "            time_diff = 0\n",
    "        res.loc[len(res)] = [curr.id, time_diff]\n",
    "    X['time_diff'] = pd.merge(X, res, how = 'left')['time_diff']\n",
    "    \n",
    "    \n",
    "# ВСЕ ПРЕОБРАЗОВАНИЯ\n",
    "def apply_technicals(X):\n",
    "    get_time_difference(X)\n",
    "    concat_all_text(X)\n",
    "    encode_date(X)\n",
    "    process_coments(X)\n",
    "    get_assignee_info(X)\n",
    "    \n",
    "    cat_cols = ['position', 'hiring_type', 'payment_type', 'project_id']\n",
    "    for column in cat_cols:\n",
    "        make_categorical(X, column)\n",
    "    \n",
    "    #vals = [preprocess(word) for word in X.all_text]\n",
    "    #get_tf_idf_enc(vals, X, 'summary')\n",
    "    \n",
    "    for field in ['assignee_id', 'creator_id']:\n",
    "        get_stats(df_all, field)\n",
    "        \n",
    "    X['key'] = X.key.apply(lambda x: x.split('-')[1]).astype('int')        \n",
    "    X['is_self_assigned'] = np.where(X.assignee_id == X.creator_id, 1, 0)\n",
    "    X['len_summary'] = X['summary'].apply(len)\n",
    "    #get_bert_enc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "80cd5f21-c024-4f96-97ec-a2bc4552afba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting time differences since last task...\n",
      "processisng date...\n",
      "processisng comments...\n",
      "processisng assignee info...\n",
      "getting assignee statistics...\n",
      "getting creator statistics...\n"
     ]
    }
   ],
   "source": [
    "apply_technicals(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "7fa2b9b1-44d3-483d-9eae-01ca3bcaf80f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "high_targ = unique_words(lower=60000)\n",
    "rest = unique_words(upper=60000)\n",
    "merged = pd.merge(high_targ, rest, left_on='word', right_on='word', how='left').fillna(0.001)\n",
    "merged = merged[merged.freq_x / merged.freq_y >= 2].sort_values('freq_x', ascending=False)\n",
    "\n",
    "df_all['clean_text'] = df_all.summary.apply(preprocess)\n",
    "df_all['one_more_feature'] = df_all.clean_text.apply(check_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "94a15edb-1129-475a-b869-611277b4dc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122, 3)"
      ]
     },
     "execution_count": 925,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "50204780-cca1-4bcd-8600-8fb5771b5a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bert_umap_0', 'bert_umap_1', 'bert_umap_2', 'bert_umap_3',\n",
       "       'bert_umap_4', 'summary_umap_0', 'summary_umap_1', 'summary_umap_2',\n",
       "       'summary_umap_3', 'summary_umap_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df = pd.read_csv('data/embeddings-1.csv', index_col=0)\n",
    "embeddings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "id": "837359a7-1615-4b8f-bd8a-8cbd7a835ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all, embeddings_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "b835eb6a-ba83-48f7-92e1-0d046efb8098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'key', 'summary', 'project_id', 'assignee_id', 'creator_id',\n",
       "       'overall_worklogs', 'time_diff', 'all_text', 'year', 'day', 'month',\n",
       "       'hour', 'day_of_week', 'comments_count', 'comments_len', 'position',\n",
       "       'hiring_type', 'payment_type', 'passport', 'is_labor_contract_signed',\n",
       "       'is_added_one_to_one', 'assignee_min_time', 'assignee_max_time',\n",
       "       'assignee_mean_time', 'assignee_median_time', 'creator_min_time',\n",
       "       'creator_max_time', 'creator_mean_time', 'creator_median_time',\n",
       "       'is_self_assigned', 'len_summary', 'clean_text', 'one_more_feature',\n",
       "       'bert_umap_0', 'bert_umap_1', 'bert_umap_2', 'bert_umap_3',\n",
       "       'bert_umap_4', 'summary_umap_0', 'summary_umap_1', 'summary_umap_2',\n",
       "       'summary_umap_3', 'summary_umap_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908af8eb-81e3-4138-884f-56cae3afda98",
   "metadata": {},
   "source": [
    "## Разбивка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "5bae5dd0-9216-4332-bb2e-4a00104e05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_todrop = ['overall_worklogs', 'id', 'summary', 'clean_text', 'all_text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "62ac5210-a5f4-4a30-a9ae-deca40865eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_all[df_all.overall_worklogs == -1].copy()\n",
    "df_test.drop(features_todrop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "c9eaef7b-5609-43df-a2c3-9105ce74accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all[(df_all.overall_worklogs != -1) & (df_all.overall_worklogs < 2000000)].copy()\n",
    "y = df_train.overall_worklogs\n",
    "y = np.log(y+0.001)\n",
    "df_train.drop(features_todrop, axis=1, inplace=True)\n",
    "X = df_train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be58bf3-60b3-4a17-9438-ef29d05c6a95",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "id": "b52526ee-a284-47b4-9d79-905af8d16a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_r2(y_true, y_pred):\n",
    "    return 'r2', r2_score(y_true, y_pred), True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "id": "fd06a6e3-39a0-46e7-8715-697090ec17f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain's l2: 0.405251\ttrain's r2: 0.732036\tval's l2: 1.04342\tval's r2: 0.320761\n",
      "[400]\ttrain's l2: 0.190179\ttrain's r2: 0.874248\tval's l2: 1.06862\tval's r2: 0.304357\n",
      "[600]\ttrain's l2: 0.0969792\ttrain's r2: 0.935875\tval's l2: 1.08957\tval's r2: 0.290718\n",
      "[800]\ttrain's l2: 0.0512606\ttrain's r2: 0.966105\tval's l2: 1.10265\tval's r2: 0.282204\n",
      "[1000]\ttrain's l2: 0.0274517\ttrain's r2: 0.981848\tval's l2: 1.10904\tval's r2: 0.278044\n",
      "R2 score: 0.25354419506711345\n"
     ]
    }
   ],
   "source": [
    "params = {'num_leaves': 30, 'n_estimators': 1000, 'learning_rate': 0.1} #0.12 0.21\n",
    "lgbm = lightgbm.LGBMRegressor(**params)\n",
    "lgbm = lgbm.fit(X_train, y_train, \n",
    "                eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                eval_names=['train', 'val'], eval_metric=lgbm_r2, \n",
    "                callbacks=[lightgbm.log_evaluation(200)])\n",
    "res_val_df = pd.DataFrame(np.exp(y_test))\n",
    "res_val_df['pred'] = np.exp(lgbm.predict(X_test))\n",
    "\n",
    "#res_val_df = pd.DataFrame(y_test)\n",
    "#res_val_df['pred'] = lgbm.predict(X_test)\n",
    "#res_val_df['pred'] = np.where(res_val_df.pred<0, 5000, res_val_df.pred)\n",
    "print(f'R2 score: {r2_score(res_val_df.overall_worklogs, res_val_df.pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "id": "a8e796ee-5630-454c-b7f3-6655c5bae619",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 scores:\n",
      "0.37686067262510803\n",
      "0.20522227393368486\n",
      "0.2202491627480354\n",
      "0.3304453168914895\n",
      "0.3012850398050845\n",
      "0.10015086831432185\n",
      "0.13667726713878936\n",
      "0.21912900050185846\n",
      "0.03167001743579778\n",
      "0.047126403913792525\n",
      "Mean R2: 0.19688160233079627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, random_state=RANDOM_STATE, shuffle=True)\n",
    "CV_res = []\n",
    "splits = {}\n",
    "counter = 0\n",
    "for trn_idx, tst_idx in kf.split(df_train):\n",
    "    X_train1, X_test1, y_train1, y_test1 = X.iloc[trn_idx], X.iloc[tst_idx], y.iloc[trn_idx], y.iloc[tst_idx]\n",
    "    lgbm = lightgbm.LGBMRegressor(**params)\n",
    "    lgbm = lgbm.fit(X_train1, y_train1)\n",
    "    CV_res.append(r2_score(np.exp(y_test1), np.exp(lgbm.predict(X_test1))))\n",
    "    #CV_res.append(r2_score(y_test1, lgbm.predict(X_test1)))\n",
    "    splits[counter] = [trn_idx, tst_idx]\n",
    "    counter += 1\n",
    "print('R2 scores:')\n",
    "for i in CV_res: print(i)\n",
    "print(f'Mean R2: {np.mean(CV_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "596ffa0e-6232-480d-8864-e9e735bf6f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_worklogs</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>2397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14604.406507</td>\n",
       "      <td>10541.346980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33718.880108</td>\n",
       "      <td>17619.555819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>60.001000</td>\n",
       "      <td>403.063441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3000.001000</td>\n",
       "      <td>4243.209513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7200.001000</td>\n",
       "      <td>6613.470818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14400.001000</td>\n",
       "      <td>10375.002708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>594600.001000</td>\n",
       "      <td>273231.799242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall_worklogs           pred\n",
       "count       2397.000000    2397.000000\n",
       "mean       14604.406507   10541.346980\n",
       "std        33718.880108   17619.555819\n",
       "min           60.001000     403.063441\n",
       "25%         3000.001000    4243.209513\n",
       "50%         7200.001000    6613.470818\n",
       "75%        14400.001000   10375.002708\n",
       "max       594600.001000  273231.799242"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_val_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "df5bb815-509e-481c-ad28-e51a6e58bca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_diff                   2145\n",
       "summary_umap_0              1680\n",
       "bert_umap_4                 1676\n",
       "bert_umap_3                 1673\n",
       "summary_umap_1              1646\n",
       "summary_umap_4              1597\n",
       "summary_umap_3              1572\n",
       "len_summary                 1553\n",
       "summary_umap_2              1495\n",
       "bert_umap_1                 1494\n",
       "key                         1433\n",
       "bert_umap_0                 1400\n",
       "day                         1294\n",
       "bert_umap_2                 1237\n",
       "hour                         952\n",
       "comments_len                 729\n",
       "day_of_week                  552\n",
       "month                        533\n",
       "assignee_max_time            506\n",
       "creator_max_time             504\n",
       "creator_id                   473\n",
       "assignee_id                  453\n",
       "creator_mean_time            412\n",
       "assignee_mean_time           405\n",
       "creator_median_time          284\n",
       "assignee_median_time         232\n",
       "comments_count               223\n",
       "creator_min_time             135\n",
       "position                     134\n",
       "is_self_assigned             114\n",
       "assignee_min_time            113\n",
       "year                          91\n",
       "payment_type                  81\n",
       "project_id                    49\n",
       "is_labor_contract_signed      33\n",
       "passport                      33\n",
       "one_more_feature              31\n",
       "hiring_type                   19\n",
       "is_added_one_to_one           14\n",
       "dtype: int32"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_feat = pd.Series(lgbm.feature_importances_, index=X_train.columns)\n",
    "lgbm_feat.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31af09-090d-4e0c-b9ba-b68688807076",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "1fac2621-067e-4300-bcd3-943c85c82b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lgbm.predict(df_test)\n",
    "pred = np.exp(pred)\n",
    "pred = np.where(pred<0, 600, pred)\n",
    "solution = pd.DataFrame({'overall_worklogs': pred}, index=df_issues_test.id)\n",
    "solution.to_csv('data/solution_58.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "2009d0bd-ad72-43f4-859c-26d40bd7e5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key\n",
      "project_id\n",
      "assignee_id\n",
      "creator_id\n",
      "time_diff\n",
      "year\n",
      "day\n",
      "month\n",
      "hour\n",
      "day_of_week\n",
      "comments_count\n",
      "comments_len\n",
      "position\n",
      "hiring_type\n",
      "payment_type\n",
      "passport\n",
      "is_labor_contract_signed\n",
      "is_added_one_to_one\n",
      "assignee_min_time\n",
      "assignee_max_time\n",
      "assignee_mean_time\n",
      "assignee_median_time\n",
      "creator_min_time\n",
      "creator_max_time\n",
      "creator_mean_time\n",
      "creator_median_time\n",
      "is_self_assigned\n",
      "len_summary\n",
      "one_more_feature\n",
      "bert_umap_0\n",
      "bert_umap_1\n",
      "bert_umap_2\n",
      "bert_umap_3\n",
      "bert_umap_4\n",
      "summary_umap_0\n",
      "summary_umap_1\n",
      "summary_umap_2\n",
      "summary_umap_3\n",
      "summary_umap_4\n"
     ]
    }
   ],
   "source": [
    "for i in X_train.columns: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ee8db-3398-4316-b8d0-ee83533fe5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
