{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb08bc7-1ea9-4860-8679-ce522ada7b5a",
   "metadata": {},
   "source": [
    "## Библиотеки и глобальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac88efaa-47a2-4c3b-8823-4b77f3569dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roman\\Miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Roman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pymorphy2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import Counter\n",
    "import lightgbm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from umap.umap_ import UMAP\n",
    "import torch\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('russian') + stopwords.words('english')\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04220ca-c32a-41a7-b1f0-7f6a22c80f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "BERT = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "MORPH = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e1c47-949e-43ac-919c-bc71eadf62c5",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "03378fb0-ae0e-4270-8b6d-1fbbd3e6ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues_train = pd.read_csv(\"data/train_issues.csv\")\n",
    "df_comment_train = pd.read_csv(\"data/train_comments.csv\")\n",
    "\n",
    "df_issues_test = pd.read_csv(\"data/test_issues.csv\")\n",
    "df_comment_test = pd.read_csv(\"data/test_comments.csv\")\n",
    "\n",
    "df_emp = pd.read_csv(\"data/employees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "34cffd58-5a23-4471-92d2-9f86a62394dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues_test['overall_worklogs'] = -1\n",
    "df_all = pd.concat([df_issues_train, df_issues_test]).reset_index(drop=True)\n",
    "df_all_com = pd.concat([df_comment_train, df_comment_test]).reset_index(drop=True)\n",
    "df_all_com.text = df_all_com.text.apply(lambda x: x.split(':')[-1] if x.__contains__('mentioned this issue in') else x) # Чистим комментарии от автотекста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10a2d2-d11a-47a3-9534-757fd96850af",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "886bd3c1-c28e-40bf-a156-e4993dc24061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(lower=0, upper=df_issues_train.overall_worklogs.max()):\n",
    "    X = df_issues_train[df_issues_train.overall_worklogs.between(lower, upper)].copy()\n",
    "    X['clean_text'] = X.summary.apply(preprocess)\n",
    "    vocab = get_vocab(X.clean_text)\n",
    "    vocab = pd.DataFrame({'word': vocab.keys(), 'freq': vocab.values()})\n",
    "    \n",
    "    all_txt = []\n",
    "    texts = [all_txt + i for i in X.clean_text]\n",
    "    freqs=[]\n",
    "    for lst in texts:\n",
    "        freqs +=lst\n",
    "    freqs = Counter(freqs)\n",
    "    freqs = pd.DataFrame({'word': freqs.keys(), 'freq': freqs.values()})\n",
    "    freqs = freqs[~freqs.word.isin(stop_words)]\n",
    "    return freqs.copy()\n",
    "\n",
    "def check_unique_words(lst):\n",
    "    for word in lst:\n",
    "        if word in merged.word.values:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "b2da83b5-a95f-4aba-9139-bf79dcdf33a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# УНИКАЛЬНЫЕ СЛОВА В ЗАДАНИЯХ С ВЫСОКИМ ТАРГЕТОМ\n",
    "high_targ = unique_words(lower=100000)\n",
    "rest = unique_words(upper=100000)\n",
    "merged = pd.merge(high_targ, rest, left_on='word', right_on='word', how='left').fillna(0.001)\n",
    "merged = merged[merged.freq_x / merged.freq_y >= 1.5].sort_values('freq_x', ascending=False)\n",
    "\n",
    "df_all['clean_text'] = df_all.summary.apply(preprocess)\n",
    "df_all['one_more_feature'] = df_all.clean_text.apply(check_unique_words)\n",
    "df_all.drop('clean_text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "cb4a26f3-a983-4c0d-804d-67eb7033e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОБЪЕДИНЕНИЕ ВСЕХ ТЕКСТОВЫХ ДАННЫХ\n",
    "def concat_all_text(X):    \n",
    "    com_df = pd.merge(X, df_all_com, left_on=\"id\", right_on=\"issue_id\", how='left')\n",
    "    com_df.text = com_df.text.fillna('')\n",
    "    all_text = {}\n",
    "    for val in X.id:\n",
    "        all_text[val] = com_df[com_df.id == val].summary.iloc[0]\n",
    "        all_text[val] += ' '.join([text for text in com_df[com_df.id == val].text.values])\n",
    "    X['all_text'] = all_text.values()\n",
    "    \n",
    "# ДАТА И ВРЕМЯ\n",
    "def encode_date(X):\n",
    "    X.created = pd.to_datetime(X.created)\n",
    "    X['year'] = X.created.dt.year\n",
    "    X['day'] = X.created.dt.day\n",
    "    X['month'] = X.created.dt.month\n",
    "    X['hour'] = X.created.dt.hour\n",
    "    X['day_of_week'] = X.created.dt.strftime(\"%w\").astype(int)\n",
    "    X.drop('created', axis=1, inplace=True)\n",
    "\n",
    "# КОЛИЧЕСТВО И ДЛИНА КОММЕНТАРИЕВ\n",
    "def process_coments(X):\n",
    "    com_df = pd.DataFrame(pd.merge(X, df_all_com, left_on=\"id\", right_on=\"issue_id\", how='left'))\n",
    "    com_df.text = com_df.text.astype('str')\n",
    "    com_df.text = com_df.text.apply(len).astype('int')\n",
    "    counts = com_df.groupby('id').comment_id.count()\n",
    "    lens = com_df.groupby('id').text.sum()\n",
    "    X['comments_count'] = pd.merge(X, counts, left_on='id', right_index=True).comment_id\n",
    "    X['comments_len'] = pd.merge(X, lens, left_on='id', right_index=True).text\n",
    "    \n",
    "# ИНФОРМАЦИЯ ОБ ИСПОЛНИТЕЛЕ\n",
    "def get_assignee_info(X):\n",
    "    df = pd.merge(X, df_emp, left_on='assignee_id', right_on='id', how='left')\n",
    "    df.fillna('unknown', inplace=True)\n",
    "    col_list = ['position', 'hiring_type', 'payment_type']\n",
    "    X[col_list] = df[col_list]\n",
    "\n",
    "# СТАТИСТИКИ СОТРУДНИКОВ\n",
    "def get_stats(X, field):\n",
    "    train_df = X[X.overall_worklogs != -1]\n",
    "    stats_df = train_df.groupby(field).agg({'overall_worklogs': [min, max, np.mean, np.median]})\n",
    "    stats_df.columns = [f'{field.split(\"_\")[0]}_{i}_time' for i in stats_df.columns.droplevel(0)]\n",
    "    X[stats_df.columns] = pd.merge(X, stats_df, left_on=field, right_index=True, how='left')[stats_df.columns]\n",
    "    \n",
    "# TF-IDF UMAP\n",
    "def preprocess(text: str) -> list:\n",
    "    tokenized = nltk.word_tokenize(text.lower())\n",
    "    normalized = [MORPH.parse(word)[0].normal_form for word in tokenized\n",
    "                  if len(word) > 1 and word.isalpha()]\n",
    "    return normalized\n",
    "\n",
    "def tf_idf(freqs):\n",
    "    return (freqs / (freqs.sum(axis=1) + 0.0001)[:, np.newaxis] ) * np.log(freqs.shape[0] / ((freqs >= 1).sum(axis=0) + 1))\n",
    "\n",
    "def mess_to_vec(vocab: dict, text):\n",
    "    vec = np.zeros(len(vocab) + 1)\n",
    "    for token in text:\n",
    "        if token in vocab:\n",
    "            vec[vocab[token]] += 1\n",
    "        else:\n",
    "            vec[0] += 1\n",
    "    return vec\n",
    "\n",
    "def get_vocab(messages):\n",
    "    vocab = {}\n",
    "    pos = 1\n",
    "    cc = Counter()\n",
    "    for message in messages:\n",
    "        n_cc = Counter(message)\n",
    "        for key in n_cc:\n",
    "            n_cc[key] = 1\n",
    "        cc += n_cc\n",
    "    for key, val in cc.items():\n",
    "        if val > 1:\n",
    "            vocab[key] = pos\n",
    "            pos += 1\n",
    "    return vocab\n",
    "\n",
    "def get_tf_idf_enc(vals, new_df, col):\n",
    "    vocab = get_vocab(vals)\n",
    "    all_freq = []\n",
    "    for val in vals:\n",
    "        all_freq.append(mess_to_vec(vocab, val))\n",
    "    all_freq = np.array(all_freq)\n",
    "    \n",
    "    all_freq = tf_idf(all_freq)\n",
    "    reduct = UMAP(densmap=True,\n",
    "                  dens_lambda=1.,\n",
    "                  n_neighbors=60,\n",
    "                  min_dist=0.1,\n",
    "                  n_components=3,\n",
    "                  random_state=42,\n",
    "                  low_memory=False,\n",
    "                  metric='euclidean',\n",
    "                  output_metric='euclidean'\n",
    "                  )\n",
    "\n",
    "    umap_res = reduct.fit_transform(all_freq).T\n",
    "    for i in range(len(umap_res)):\n",
    "        new_df[col + f'_umap_{i}'] = umap_res[i]\n",
    "\n",
    "# BERT UMAP\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "def get_bert_enc(X):   \n",
    "    embeddings = []\n",
    "    print('Making BERT embeddings...')\n",
    "    for sentence in tqdm(X['all_text']):\n",
    "        embeddings.append(embed_bert_cls(sentence, BERT, TOKENIZER))\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    reduct = UMAP(densmap=True,\n",
    "                  dens_lambda=1.,\n",
    "                  n_neighbors=60,\n",
    "                  min_dist=0.1,\n",
    "                  n_components=3,\n",
    "                  random_state=42,\n",
    "                  low_memory=False,\n",
    "                  metric='euclidean',\n",
    "                  output_metric='euclidean'\n",
    "                  )\n",
    "\n",
    "    umap_res = reduct.fit_transform(embeddings).T\n",
    "    for i in range(len(umap_res)):\n",
    "        X[f'bert_umap_{i}'] = umap_res[i]\n",
    "\n",
    "# КАТЕГОРИАЛЬНЫЕ ПРИЗНАКИ\n",
    "def make_categorical(df, col):\n",
    "    df[col] = pd.Categorical(df[col])\n",
    "    df[col] = df[col].astype('category').cat.codes\n",
    "\n",
    "# ПОДСЧЕТ КОЛИЧЕСТВА ТЕКУЩИХ ЗАЯВОК У ИСПОЛНИТЕЛЯ\n",
    "def get_current_tasks_count(X):    \n",
    "    res = []\n",
    "    X.created = pd.to_datetime(X.created)\n",
    "    for df in tqdm(X.expanding()):\n",
    "        curr = df.iloc[-1]\n",
    "        tasks_count = 0\n",
    "        for task in df[df.assignee_id == curr.assignee_id].itertuples():\n",
    "            if pd.to_datetime(task.created) + pd.Timedelta(task.overall_worklogs, 'sec') > curr.created:\n",
    "                tasks_count += 1\n",
    "        res.append(tasks_count)\n",
    "    X['curr_tasks_count'] = res\n",
    "    \n",
    "# ВСЕ ПРЕОБРАЗОВАНИЯ\n",
    "def apply_technicals(X):\n",
    "    concat_all_text(X)\n",
    "    get_current_tasks_count(X)\n",
    "    encode_date(X)\n",
    "    process_coments(X)\n",
    "    get_assignee_info(X)\n",
    "    \n",
    "    cat_cols = ['position', 'hiring_type', 'payment_type']\n",
    "    for column in cat_cols:\n",
    "        make_categorical(X, column)\n",
    "    \n",
    "    vals = [preprocess(word) for word in X.summary]\n",
    "    get_tf_idf_enc(vals, X, 'summary')\n",
    "    \n",
    "    for field in ['assignee_id', 'creator_id', 'project_id']:\n",
    "        get_stats(df_all, field)\n",
    "    \n",
    "    X['is_self_assigned'] = np.where(X.assignee_id == X.creator_id, 1, 0)\n",
    "    X['len_summary'] = X['summary'].apply(len)\n",
    "    get_bert_enc(X)\n",
    "    X.drop(['key', 'id', 'summary', 'all_text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "80cd5f21-c024-4f96-97ec-a2bc4552afba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10659it [03:41, 48.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10659/10659 [09:54<00:00, 17.93it/s]\n"
     ]
    }
   ],
   "source": [
    "apply_technicals(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908af8eb-81e3-4138-884f-56cae3afda98",
   "metadata": {},
   "source": [
    "## Разбивка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "62ac5210-a5f4-4a30-a9ae-deca40865eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_all[df_all.overall_worklogs == -1].copy()\n",
    "df_test.drop('overall_worklogs', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "id": "c9eaef7b-5609-43df-a2c3-9105ce74accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all[(df_all.overall_worklogs != -1) & (df_all.overall_worklogs < 2000000)].copy()\n",
    "\n",
    "y = df_train.overall_worklogs\n",
    "y = np.log(y)\n",
    "X = df_train.drop('overall_worklogs', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be58bf3-60b3-4a17-9438-ef29d05c6a95",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "b52526ee-a284-47b4-9d79-905af8d16a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_r2(y_true, y_pred):\n",
    "    return 'r2', r2_score(y_true, y_pred), True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "fd06a6e3-39a0-46e7-8715-697090ec17f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain's l2: 0.839416\ttrain's r2: 0.444954\tval's l2: 1.07405\tval's r2: 0.300821\n",
      "[200]\ttrain's l2: 0.670258\ttrain's r2: 0.556806\tval's l2: 1.07024\tval's r2: 0.303304\n",
      "[300]\ttrain's l2: 0.550616\ttrain's r2: 0.635917\tval's l2: 1.06931\tval's r2: 0.303905\n",
      "R2 score: 0.2905315976172511\n"
     ]
    }
   ],
   "source": [
    "params = {'num_leaves': 30, 'n_estimators': 300, 'learning_rate': 0.05}\n",
    "lgbm = lightgbm.LGBMRegressor(**params)\n",
    "lgbm = lgbm.fit(X_train, y_train, \n",
    "                eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                eval_names=['train', 'val'], eval_metric=lgbm_r2, \n",
    "                callbacks=[lightgbm.log_evaluation(100)])\n",
    "res_val_df = pd.DataFrame(np.exp(y_test))\n",
    "res_val_df['pred'] = np.exp(lgbm.predict(X_test))\n",
    "\n",
    "print(f'R2 score: {r2_score(res_val_df.overall_worklogs, res_val_df.pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "a8e796ee-5630-454c-b7f3-6655c5bae619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 scores:\n",
      "0.17253208494927108\n",
      "0.10776343357248486\n",
      "0.35741388932455975\n",
      "0.07876192839302121\n",
      "0.09405300114607851\n",
      "0.22810330156247405\n",
      "0.10397750976820408\n",
      "0.32624141568296394\n",
      "0.23811946567277809\n",
      "0.11254193917136945\n",
      "Mean R2: 0.1819507969243205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, random_state=40, shuffle=True)\n",
    "CV_res = []\n",
    "splits = {}\n",
    "counter = 0\n",
    "for trn_idx, tst_idx in kf.split(df_train):\n",
    "    X_train1, X_test1, y_train1, y_test1 = X.iloc[trn_idx], X.iloc[tst_idx], y.iloc[trn_idx], y.iloc[tst_idx]\n",
    "    lgbm = lightgbm.LGBMRegressor(**params)\n",
    "    lgbm = lgbm.fit(X_train1, y_train1)\n",
    "    CV_res.append(r2_score(np.exp(y_test1), np.exp(lgbm.predict(X_test1))))\n",
    "    splits[counter] = [trn_idx, tst_idx]\n",
    "    counter += 1\n",
    "print('R2 scores:')\n",
    "for i in CV_res: print(i)\n",
    "print(f'Mean R2: {np.mean(CV_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "596ffa0e-6232-480d-8864-e9e735bf6f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_worklogs</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>2397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14604.405507</td>\n",
       "      <td>9823.580404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33718.880108</td>\n",
       "      <td>18295.597442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>588.415102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3000.000000</td>\n",
       "      <td>4650.172485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7200.000000</td>\n",
       "      <td>6614.388602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14400.000000</td>\n",
       "      <td>9644.453471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>594600.000000</td>\n",
       "      <td>582549.157247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall_worklogs           pred\n",
       "count       2397.000000    2397.000000\n",
       "mean       14604.405507    9823.580404\n",
       "std        33718.880108   18295.597442\n",
       "min           60.000000     588.415102\n",
       "25%         3000.000000    4650.172485\n",
       "50%         7200.000000    6614.388602\n",
       "75%        14400.000000    9644.453471\n",
       "max       594600.000000  582549.157247"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_val_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "id": "df5bb815-509e-481c-ad28-e51a6e58bca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary_umap_0          957\n",
       "summary_umap_2          953\n",
       "summary_umap_1          936\n",
       "bert_umap_1             907\n",
       "bert_umap_2             904\n",
       "bert_umap_0             872\n",
       "day                     711\n",
       "len_summary             707\n",
       "hour                    519\n",
       "month                   463\n",
       "comments_len            443\n",
       "curr_tasks_count        432\n",
       "creator_max_time        281\n",
       "day_of_week             277\n",
       "assignee_mean_time      235\n",
       "creator_id              230\n",
       "assignee_max_time       214\n",
       "assignee_id             203\n",
       "comments_count          188\n",
       "creator_mean_time       178\n",
       "creator_median_time     171\n",
       "assignee_median_time    168\n",
       "assignee_min_time       110\n",
       "is_self_assigned         93\n",
       "creator_min_time         90\n",
       "position                 80\n",
       "year                     70\n",
       "one_more_feature         57\n",
       "payment_type             43\n",
       "project_max_time         36\n",
       "project_id               26\n",
       "project_mean_time        24\n",
       "hiring_type              14\n",
       "project_median_time       8\n",
       "project_min_time          0\n",
       "dtype: int32"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_feat = pd.Series(lgbm.feature_importances_, index=X_train.columns)\n",
    "lgbm_feat.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31af09-090d-4e0c-b9ba-b68688807076",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "1fac2621-067e-4300-bcd3-943c85c82b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lgbm.predict(df_test)\n",
    "pred = np.exp(pred)\n",
    "solution = pd.DataFrame({'overall_worklogs': pred}, index=df_issues_test.id)\n",
    "solution.to_csv('data/solution_43.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbbe4f-6241-41e9-a2f2-83e7cb09d7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
